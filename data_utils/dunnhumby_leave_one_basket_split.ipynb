{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ta-Feng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import random\n",
    "from pandas.core.frame import DataFrame\n",
    "import sklearn\n",
    "from unigramTable import UnigramTable\n",
    "\n",
    "\n",
    "DEFAULT_USER_COL = \"user_ids\"\n",
    "DEFAULT_ITEM_COL = \"item_ids\"\n",
    "DEFAULT_ORDER_COL = \"order_ids\"\n",
    "DEFAULT_RATING_COL = \"ratings\"\n",
    "DEFAULT_LABEL_COL = \"label\"\n",
    "DEFAULT_TIMESTAMP_COL = \"timestamp\"\n",
    "DEFAULT_PREDICTION_COL = \"prediction\"\n",
    "DEFAULT_FLAG_COL = \"flag\"\n",
    "data_base_dir = \"../datasets/tafeng/\"\n",
    "\n",
    "negative_size = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load full data to sequence DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prepro_to_seq(data_base_dir):\n",
    "    transaction_data = data_base_dir + \"transaction_data.csv\"\n",
    "    prior_transaction = pd.read_csv(\n",
    "        transaction_data,\n",
    "        usecols=[\"BASKET_ID\", \"household_key\", \"PRODUCT_ID\", \"DAY\", \"TRANS_TIME\"],\n",
    "    )\n",
    "\n",
    "    prior_transaction[\"DAY\"] = prior_transaction[\"DAY\"].astype(str)  #\n",
    "    prior_transaction[\"TRANS_TIME\"] = prior_transaction[\"TRANS_TIME\"].astype(str)\n",
    "\n",
    "    prior_transaction[\"time\"] = (\n",
    "        prior_transaction[\"DAY\"] + prior_transaction[\"TRANS_TIME\"]\n",
    "    )\n",
    "    prior_transaction[\"time\"] = prior_transaction[\"time\"].astype(int)  #\n",
    "    prior_transaction.reset_index(inplace=True)\n",
    "    prior_transaction = prior_transaction.sort_values(by=\"time\", ascending=False)\n",
    "\n",
    "    prior_transaction.drop([\"DAY\", \"TRANS_TIME\"], axis=1)\n",
    "\n",
    "    prior_transaction = prior_transaction[\n",
    "        [\"BASKET_ID\", \"household_key\", \"PRODUCT_ID\", \"time\"]\n",
    "    ]\n",
    "    prior_transaction.insert(3, \"flag\", \"train\")\n",
    "    prior_transaction.insert(4, \"ratings\", 1)\n",
    "    prior_transaction.rename(\n",
    "        columns={\n",
    "            \"BASKET_ID\": DEFAULT_ORDER_COL,\n",
    "            \"household_key\": DEFAULT_USER_COL,\n",
    "            \"PRODUCT_ID\": DEFAULT_ITEM_COL,\n",
    "            \"flag\": DEFAULT_FLAG_COL,\n",
    "            \"ratings\": DEFAULT_RATING_COL,\n",
    "            \"time\": DEFAULT_TIMESTAMP_COL,\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    print(\"loading raw data completed\")\n",
    "    return prior_transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw data address: http://www.dunnhumby.com/careers/engineering/sourcefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_dir = \"../datasets/dunnhumby/\"\n",
    "full_data = data_prepro_to_seq(\n",
    "    data_base_dir + \"raw/dunnhumby_The-Complete-Journey/csv/\"\n",
    ")\n",
    "full_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Row data staticstics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_interact = len(full_data.index)\n",
    "n_orders = full_data[DEFAULT_ORDER_COL].nunique()\n",
    "n_users = full_data[DEFAULT_USER_COL].nunique()\n",
    "n_items = full_data[DEFAULT_ITEM_COL].nunique()\n",
    "(n_interact, n_orders, n_users, n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save these sequence data to csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the seqence data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the integrity of the saved data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_interact = len(full_data.index)\n",
    "n_orders = full_data[DEFAULT_ORDER_COL].nunique()\n",
    "n_users = full_data[DEFAULT_USER_COL].nunique()\n",
    "n_items = full_data[DEFAULT_ITEM_COL].nunique()\n",
    "(n_interact, n_orders, n_users, n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_neg_sample(eval_df, negative_num, item_sampler):\n",
    "    print(\"sampling negative items...\")\n",
    "    interact_status = eval_df.groupby([\"user_ids\"])[\"item_ids\"].apply(set).reset_index()\n",
    "    total_interact = pd.DataFrame(\n",
    "        {\"user_ids\": [], \"item_ids\": [], \"ratings\": []}, dtype=np.long\n",
    "    )\n",
    "    for index, user_items in interact_status.iterrows():\n",
    "        u = int(user_items[\"user_ids\"])\n",
    "        items = set(user_items[\"item_ids\"])  # item set for user u\n",
    "        n_items = len(items)  # number of positive item for user u\n",
    "        sample_neg_items = set(\n",
    "            item_sampler.sample(negative_num + n_items, 1, True)\n",
    "        )  # first sample negative_num+n_items items\n",
    "        sample_neg_items = list(sample_neg_items - items)[:negative_num]\n",
    "        # filter the positive items and truncate the first negative_num\n",
    "        #     print(len(sample_neg_items))\n",
    "        tp_items = np.append(list(items), sample_neg_items)\n",
    "        #     print(len(tp_items))\n",
    "\n",
    "        tp_users = np.array([1] * (negative_num + n_items), dtype=np.long) * u\n",
    "        tp_ones = np.ones(n_items, dtype=np.long)\n",
    "        tp_zeros = np.zeros(negative_num, dtype=np.long)\n",
    "        ratings = np.append(tp_ones, tp_zeros)\n",
    "        #     print(len(tp_users)),print(len(tp_items)),print(len(ratings))\n",
    "        tp = pd.DataFrame(\n",
    "            {\"user_ids\": tp_users, \"item_ids\": tp_items, \"ratings\": ratings}\n",
    "        )\n",
    "        total_interact = total_interact.append(tp)\n",
    "\n",
    "    total_interact = sklearn.utils.shuffle(total_interact)\n",
    "    return total_interact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  divide data into train, validata and test sets, where validata set is in the train set  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter data by count of users, items and orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by group_col and filter filter_col that has less num unique() count\n",
    "def fiter_by_count(tp, group_col, filter_col, num):\n",
    "    ordercount = (\n",
    "        tp.groupby([group_col])[filter_col].nunique().rename(\"count\").reset_index()\n",
    "    )\n",
    "    filter_tp = tp[\n",
    "        tp[group_col].isin(ordercount[ordercount[\"count\"] >= num][group_col])\n",
    "    ]\n",
    "    return filter_tp\n",
    "\n",
    "\n",
    "# filter data by the minimum purcharce number of items and users\n",
    "def filter_triplets(tp, min_u_c=5, min_i_c=5, min_o_c=5):\n",
    "    print(\"filter data by the minimum purcharce number of items and users and orders\")\n",
    "    n_interact = len(tp.index)\n",
    "    n_orders = tp[DEFAULT_ORDER_COL].nunique()\n",
    "    n_users = tp[DEFAULT_USER_COL].nunique()\n",
    "    n_items = tp[DEFAULT_ITEM_COL].nunique()\n",
    "    print(\"before filter\", n_interact, n_orders, n_users, n_items)\n",
    "    # Filter users by mixmum number of orders\n",
    "    if min_o_c > 0:\n",
    "        tp = fiter_by_count(tp, DEFAULT_USER_COL, DEFAULT_ORDER_COL, min_o_c)\n",
    "\n",
    "    # Filter users by mixmum number of items\n",
    "    if min_i_c > 0:\n",
    "        tp = fiter_by_count(tp, DEFAULT_USER_COL, DEFAULT_ITEM_COL, min_i_c)\n",
    "\n",
    "    # Filter items by mixmum number of users\n",
    "    if min_u_c > 0:\n",
    "        tp = fiter_by_count(tp, DEFAULT_ITEM_COL, DEFAULT_USER_COL, min_u_c)\n",
    "\n",
    "    n_interact = len(tp.index)\n",
    "    n_orders = tp[DEFAULT_ORDER_COL].nunique()\n",
    "    n_users = tp[DEFAULT_USER_COL].nunique()\n",
    "    n_items = tp[DEFAULT_ITEM_COL].nunique()\n",
    "    print(\"after filter\", n_interact, n_orders, n_users, n_items)\n",
    "    # Update both usercount and itemcount after filtering\n",
    "    # usercount, itemcount = get_count(tp, 'user_ids'), get_count(tp, 'item_ids')\n",
    "    return tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_to_files(data, data_base_dir):\n",
    "\n",
    "    user_ids = data[DEFAULT_USER_COL].to_numpy(dtype=np.long)\n",
    "    item_ids = data[DEFAULT_ITEM_COL].to_numpy(dtype=np.long)\n",
    "    order_ids = data[DEFAULT_ORDER_COL].to_numpy(dtype=np.long)\n",
    "    timestamps = data[DEFAULT_TIMESTAMP_COL].to_numpy(dtype=np.long)\n",
    "    ratings = data[DEFAULT_RATING_COL].to_numpy(dtype=np.float32)\n",
    "    data_file = os.path.join(data_base_dir, \"leave_one_basket\")\n",
    "    if not os.path.exists(data_file):\n",
    "        os.makedirs(data_file)\n",
    "    data_file = os.path.join(data_file, \"train.npz\")\n",
    "    np.savez_compressed(\n",
    "        data_file,\n",
    "        user_ids=user_ids,\n",
    "        item_ids=item_ids,\n",
    "        order_ids=order_ids,\n",
    "        timestamp=timestamps,\n",
    "        ratings=ratings,\n",
    "    )\n",
    "    print(\n",
    "        \"Data saving to file:\",\n",
    "        data_base_dir,\n",
    "        \"max_item_num:\",\n",
    "        np.max(item_ids),\n",
    "        \"max_user_num:\",\n",
    "        np.max(user_ids),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_test_to_files(data, data_base_dir, suffix):\n",
    "    user_ids = data[DEFAULT_USER_COL].to_numpy(dtype=np.long)\n",
    "    item_ids = data[DEFAULT_ITEM_COL].to_numpy(dtype=np.long)\n",
    "    ratings = data[DEFAULT_RATING_COL].to_numpy(dtype=np.float32)\n",
    "    data_file = os.path.join(data_base_dir, \"leave_one_basket\")\n",
    "    if not os.path.exists(data_file):\n",
    "        os.makedirs(data_file)\n",
    "    data_file = os.path.join(data_file, suffix)\n",
    "    np.savez_compressed(\n",
    "        data_file, user_ids=user_ids, item_ids=item_ids, ratings=ratings,\n",
    "    )\n",
    "    print(\n",
    "        \"Data saving to file:\",\n",
    "        data_base_dir,\n",
    "        \"max_item_num:\",\n",
    "        np.max(item_ids),\n",
    "        \"max_user_num:\",\n",
    "        np.max(user_ids),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"leave_one_basket_split\")\n",
    "from tqdm import tqdm\n",
    "\n",
    "fiter_full_data = fiter_by_count(full_data, DEFAULT_ITEM_COL, DEFAULT_USER_COL, 10)\n",
    "fiter_full_data = fiter_by_count(\n",
    "    fiter_full_data, DEFAULT_USER_COL, DEFAULT_ORDER_COL, 3\n",
    ")\n",
    "users = fiter_full_data[DEFAULT_USER_COL].unique()\n",
    "for user in tqdm(users):\n",
    "    user_data_orders = fiter_full_data[\n",
    "        fiter_full_data[DEFAULT_USER_COL] == user\n",
    "    ].sort_values(by=[\"timestamp\"], ascending=False)\n",
    "    unique_user_orders = user_data_orders[DEFAULT_ORDER_COL].unique()\n",
    "    fiter_full_data.loc[\n",
    "        fiter_full_data[DEFAULT_ORDER_COL] == unique_user_orders[0], DEFAULT_FLAG_COL,\n",
    "    ] = \"test\"\n",
    "    fiter_full_data.loc[\n",
    "        fiter_full_data[DEFAULT_ORDER_COL] == unique_user_orders[1], DEFAULT_FLAG_COL,\n",
    "    ] = \"validate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sampling negatives and saving\")\n",
    "data_base_dir = \"../../datasets/dunnhumby/\"\n",
    "\n",
    "tp_train = fiter_full_data[fiter_full_data[DEFAULT_FLAG_COL] == \"train\"]\n",
    "tp_validate = fiter_full_data[fiter_full_data[DEFAULT_FLAG_COL] == \"validate\"]\n",
    "tp_test = fiter_full_data[fiter_full_data[DEFAULT_FLAG_COL] == \"test\"]\n",
    "\n",
    "save_train_to_files(tp_train, data_base_dir)\n",
    "item_sampler = UnigramTable(tp_train[DEFAULT_ITEM_COL].value_counts().to_dict())\n",
    "\n",
    "# generate 10 different validation and testing sets\n",
    "for i in range(10):\n",
    "    tp_validate_new = feed_neg_sample(tp_validate, 100, item_sampler)\n",
    "    tp_test_new = feed_neg_sample(tp_test, 100, item_sampler)\n",
    "    save_test_to_files(tp_validate_new, data_base_dir, suffix=\"valid\" + \"_\" + str(i))\n",
    "    save_test_to_files(tp_test_new, data_base_dir, suffix=\"test\" + \"_\" + str(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
